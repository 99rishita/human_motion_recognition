{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micro_doppler import read_data, data_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## positive and neg samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973\n"
     ]
    }
   ],
   "source": [
    "data_c0 = read_data('data/walk')\n",
    "data_c1 = read_data('data/restless')\n",
    "data_c2 = read_data('data/swinghand')\n",
    "data_c3 = read_data('data/neg')\n",
    "training_data = []\n",
    "label_list = []\n",
    "tmp_1, tmp_2 = data_transform(data_c0, 0)\n",
    "training_data = training_data + tmp_1\n",
    "label_list = label_list + tmp_2\n",
    "tmp_1, tmp_2 = data_transform(data_c1, 1)\n",
    "training_data_r = training_data + tmp_1\n",
    "label_list = label_list + tmp_2 \n",
    "tmp_1, tmp_2 = data_transform(data_c2, 2)\n",
    "training_data_r = training_data + tmp_1\n",
    "label_list = label_list + tmp_2 \n",
    "tmp_1, tmp_2 = data_transform(data_c3, 3)\n",
    "training_data_r = training_data + tmp_1\n",
    "label_list = label_list + tmp_2 \n",
    "print(len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micro_doppler import ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 200\n",
    "num_classes = 4\n",
    "batch_size = 20\n",
    "learning_rate = 0.0001\n",
    "model = ConvNet(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data_r\n",
    "training_data = [[training_data[i].reshape(1,128,20), label_list[i]] for i in range(len(training_data))]\n",
    "train, test = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "training_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "testing_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "Epoch [1/200], Step [20/46], Loss: 0.9166\n",
      "Epoch [1/200], Step [40/46], Loss: 0.8262\n",
      "Epoch [2/200], Step [20/46], Loss: 0.8495\n",
      "Epoch [2/200], Step [40/46], Loss: 0.8126\n",
      "Epoch [3/200], Step [20/46], Loss: 0.7790\n",
      "Epoch [3/200], Step [40/46], Loss: 0.9754\n",
      "Epoch [4/200], Step [20/46], Loss: 0.8665\n",
      "Epoch [4/200], Step [40/46], Loss: 0.8015\n",
      "Epoch [5/200], Step [20/46], Loss: 0.8451\n",
      "Epoch [5/200], Step [40/46], Loss: 0.7963\n",
      "Epoch [6/200], Step [20/46], Loss: 0.8430\n",
      "Epoch [6/200], Step [40/46], Loss: 0.8406\n",
      "Epoch [7/200], Step [20/46], Loss: 0.7674\n",
      "Epoch [7/200], Step [40/46], Loss: 0.8318\n",
      "Epoch [8/200], Step [20/46], Loss: 0.7721\n",
      "Epoch [8/200], Step [40/46], Loss: 0.8050\n",
      "Epoch [9/200], Step [20/46], Loss: 0.7479\n",
      "Epoch [9/200], Step [40/46], Loss: 0.7535\n",
      "Epoch [10/200], Step [20/46], Loss: 0.7910\n",
      "Epoch [10/200], Step [40/46], Loss: 0.8003\n",
      "Epoch [11/200], Step [20/46], Loss: 0.7445\n",
      "Epoch [11/200], Step [40/46], Loss: 0.7455\n",
      "Epoch [12/200], Step [20/46], Loss: 0.7445\n",
      "Epoch [12/200], Step [40/46], Loss: 0.7951\n",
      "Epoch [13/200], Step [20/46], Loss: 0.7845\n",
      "Epoch [13/200], Step [40/46], Loss: 0.8367\n",
      "Epoch [14/200], Step [20/46], Loss: 0.7445\n",
      "Epoch [14/200], Step [40/46], Loss: 0.7506\n",
      "Epoch [15/200], Step [20/46], Loss: 0.7577\n",
      "Epoch [15/200], Step [40/46], Loss: 0.7517\n",
      "Epoch [16/200], Step [20/46], Loss: 0.7522\n",
      "Epoch [16/200], Step [40/46], Loss: 0.7637\n",
      "Epoch [17/200], Step [20/46], Loss: 0.7553\n",
      "Epoch [17/200], Step [40/46], Loss: 0.7439\n",
      "Epoch [18/200], Step [20/46], Loss: 0.8046\n",
      "Epoch [18/200], Step [40/46], Loss: 0.8423\n",
      "Epoch [19/200], Step [20/46], Loss: 0.7962\n",
      "Epoch [19/200], Step [40/46], Loss: 0.8530\n",
      "Epoch [20/200], Step [20/46], Loss: 0.7440\n",
      "Epoch [20/200], Step [40/46], Loss: 0.7438\n",
      "Epoch [21/200], Step [20/46], Loss: 0.7440\n",
      "Epoch [21/200], Step [40/46], Loss: 0.7939\n",
      "Epoch [22/200], Step [20/46], Loss: 0.7948\n",
      "Epoch [22/200], Step [40/46], Loss: 0.7450\n",
      "Epoch [23/200], Step [20/46], Loss: 0.7445\n",
      "Epoch [23/200], Step [40/46], Loss: 0.7943\n",
      "Epoch [24/200], Step [20/46], Loss: 0.7451\n",
      "Epoch [24/200], Step [40/46], Loss: 0.7446\n",
      "Epoch [25/200], Step [20/46], Loss: 0.7940\n",
      "Epoch [25/200], Step [40/46], Loss: 0.7455\n",
      "Epoch [26/200], Step [20/46], Loss: 0.7942\n",
      "Epoch [26/200], Step [40/46], Loss: 0.7938\n",
      "Epoch [27/200], Step [20/46], Loss: 0.7439\n",
      "Epoch [27/200], Step [40/46], Loss: 0.7438\n",
      "Epoch [28/200], Step [20/46], Loss: 0.7445\n",
      "Epoch [28/200], Step [40/46], Loss: 0.7453\n",
      "Epoch [29/200], Step [20/46], Loss: 0.7441\n",
      "Epoch [29/200], Step [40/46], Loss: 0.7939\n",
      "Epoch [30/200], Step [20/46], Loss: 0.7464\n",
      "Epoch [30/200], Step [40/46], Loss: 0.7441\n",
      "Epoch [31/200], Step [20/46], Loss: 0.7517\n",
      "Epoch [31/200], Step [40/46], Loss: 0.7455\n",
      "Epoch [32/200], Step [20/46], Loss: 0.7482\n",
      "Epoch [32/200], Step [40/46], Loss: 0.7932\n",
      "Epoch [33/200], Step [20/46], Loss: 0.7940\n",
      "Epoch [33/200], Step [40/46], Loss: 0.7438\n",
      "Epoch [34/200], Step [20/46], Loss: 0.7946\n",
      "Epoch [34/200], Step [40/46], Loss: 0.7439\n",
      "Epoch [35/200], Step [20/46], Loss: 0.7440\n",
      "Epoch [35/200], Step [40/46], Loss: 0.7462\n",
      "Epoch [36/200], Step [20/46], Loss: 0.7441\n",
      "Epoch [36/200], Step [40/46], Loss: 0.7462\n",
      "Epoch [37/200], Step [20/46], Loss: 0.7441\n",
      "Epoch [37/200], Step [40/46], Loss: 0.7439\n",
      "Epoch [38/200], Step [20/46], Loss: 0.7924\n",
      "Epoch [38/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [39/200], Step [20/46], Loss: 0.7440\n",
      "Epoch [39/200], Step [40/46], Loss: 0.7443\n",
      "Epoch [40/200], Step [20/46], Loss: 0.7440\n",
      "Epoch [40/200], Step [40/46], Loss: 0.7484\n",
      "Epoch [41/200], Step [20/46], Loss: 0.7438\n",
      "Epoch [41/200], Step [40/46], Loss: 0.7440\n",
      "Epoch [42/200], Step [20/46], Loss: 0.7940\n",
      "Epoch [42/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [43/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [43/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [44/200], Step [20/46], Loss: 0.8460\n",
      "Epoch [44/200], Step [40/46], Loss: 0.7441\n",
      "Epoch [45/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [45/200], Step [40/46], Loss: 0.7951\n",
      "Epoch [46/200], Step [20/46], Loss: 0.7465\n",
      "Epoch [46/200], Step [40/46], Loss: 0.7443\n",
      "Epoch [47/200], Step [20/46], Loss: 0.7438\n",
      "Epoch [47/200], Step [40/46], Loss: 0.7942\n",
      "Epoch [48/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [48/200], Step [40/46], Loss: 0.7471\n",
      "Epoch [49/200], Step [20/46], Loss: 0.7438\n",
      "Epoch [49/200], Step [40/46], Loss: 0.7940\n",
      "Epoch [50/200], Step [20/46], Loss: 0.7440\n",
      "Epoch [50/200], Step [40/46], Loss: 0.7443\n",
      "Epoch [51/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [51/200], Step [40/46], Loss: 0.8437\n",
      "Epoch [52/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [52/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [53/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [53/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [54/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [54/200], Step [40/46], Loss: 0.7438\n",
      "Epoch [55/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [55/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [56/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [56/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [57/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [57/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [58/200], Step [20/46], Loss: 0.7440\n",
      "Epoch [58/200], Step [40/46], Loss: 0.8437\n",
      "Epoch [59/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [59/200], Step [40/46], Loss: 0.8437\n",
      "Epoch [60/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [60/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [61/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [61/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [62/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [62/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [63/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [63/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [64/200], Step [20/46], Loss: 0.7939\n",
      "Epoch [64/200], Step [40/46], Loss: 0.7438\n",
      "Epoch [65/200], Step [20/46], Loss: 0.7564\n",
      "Epoch [65/200], Step [40/46], Loss: 0.7438\n",
      "Epoch [66/200], Step [20/46], Loss: 0.7439\n",
      "Epoch [66/200], Step [40/46], Loss: 0.7944\n",
      "Epoch [67/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [67/200], Step [40/46], Loss: 0.7953\n",
      "Epoch [68/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [68/200], Step [40/46], Loss: 0.7440\n",
      "Epoch [69/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [69/200], Step [40/46], Loss: 0.7458\n",
      "Epoch [70/200], Step [20/46], Loss: 0.7440\n",
      "Epoch [70/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [71/200], Step [20/46], Loss: 0.7526\n",
      "Epoch [71/200], Step [40/46], Loss: 0.8437\n",
      "Epoch [72/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [72/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [73/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [73/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [74/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [74/200], Step [40/46], Loss: 0.8437\n",
      "Epoch [75/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [75/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [76/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [76/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [77/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [77/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [78/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [78/200], Step [40/46], Loss: 0.7441\n",
      "Epoch [79/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [79/200], Step [40/46], Loss: 0.7438\n",
      "Epoch [80/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [80/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [81/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [81/200], Step [40/46], Loss: 0.8437\n",
      "Epoch [82/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [82/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [83/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [83/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [84/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [84/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [85/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [85/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [86/200], Step [20/46], Loss: 0.7439\n",
      "Epoch [86/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [87/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [87/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [88/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [88/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [89/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [89/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [90/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [90/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [91/200], Step [20/46], Loss: 0.7936\n",
      "Epoch [91/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [92/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [92/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [93/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [93/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [94/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [94/200], Step [40/46], Loss: 0.8437\n",
      "Epoch [95/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [95/200], Step [40/46], Loss: 0.7441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [96/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [97/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [97/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [98/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [98/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [99/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [99/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [100/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [100/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [101/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [101/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [102/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [102/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [103/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [103/200], Step [40/46], Loss: 0.7939\n",
      "Epoch [104/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [104/200], Step [40/46], Loss: 0.7445\n",
      "Epoch [105/200], Step [20/46], Loss: 0.7586\n",
      "Epoch [105/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [106/200], Step [20/46], Loss: 0.7438\n",
      "Epoch [106/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [107/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [107/200], Step [40/46], Loss: 0.7938\n",
      "Epoch [108/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [108/200], Step [40/46], Loss: 0.7440\n",
      "Epoch [109/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [109/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [110/200], Step [20/46], Loss: 0.7940\n",
      "Epoch [110/200], Step [40/46], Loss: 0.7439\n",
      "Epoch [111/200], Step [20/46], Loss: 0.8009\n",
      "Epoch [111/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [112/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [112/200], Step [40/46], Loss: 0.7957\n",
      "Epoch [113/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [113/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [114/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [114/200], Step [40/46], Loss: 0.8835\n",
      "Epoch [115/200], Step [20/46], Loss: 0.7538\n",
      "Epoch [115/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [116/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [116/200], Step [40/46], Loss: 0.7936\n",
      "Epoch [117/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [117/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [118/200], Step [20/46], Loss: 0.7919\n",
      "Epoch [118/200], Step [40/46], Loss: 0.8437\n",
      "Epoch [119/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [119/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [120/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [120/200], Step [40/46], Loss: 0.7441\n",
      "Epoch [121/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [121/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [122/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [122/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [123/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [123/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [124/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [124/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [125/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [125/200], Step [40/46], Loss: 0.7936\n",
      "Epoch [126/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [126/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [127/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [127/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [128/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [128/200], Step [40/46], Loss: 0.7438\n",
      "Epoch [129/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [129/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [130/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [130/200], Step [40/46], Loss: 0.8437\n",
      "Epoch [131/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [131/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [132/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [132/200], Step [40/46], Loss: 0.7438\n",
      "Epoch [133/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [133/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [134/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [134/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [135/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [135/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [136/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [136/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [137/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [137/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [138/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [138/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [139/200], Step [20/46], Loss: 0.8437\n",
      "Epoch [139/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [140/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [140/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [141/200], Step [20/46], Loss: 0.7939\n",
      "Epoch [141/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [142/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [142/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [143/200], Step [20/46], Loss: 0.7449\n",
      "Epoch [143/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [144/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [144/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [145/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [145/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [146/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [146/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [147/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [147/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [148/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [148/200], Step [40/46], Loss: 0.7723\n",
      "Epoch [149/200], Step [20/46], Loss: 0.7439\n",
      "Epoch [149/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [150/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [150/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [151/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [151/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [152/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [152/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [153/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [153/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [154/200], Step [20/46], Loss: 0.7446\n",
      "Epoch [154/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [155/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [155/200], Step [40/46], Loss: 0.7482\n",
      "Epoch [156/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [156/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [157/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [157/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [158/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [158/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [159/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [159/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [160/200], Step [20/46], Loss: 0.7438\n",
      "Epoch [160/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [161/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [161/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [162/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [162/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [163/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [163/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [164/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [164/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [165/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [165/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [166/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [166/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [167/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [167/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [168/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [168/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [169/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [169/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [170/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [170/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [171/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [171/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [172/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [172/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [173/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [173/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [174/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [174/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [175/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [175/200], Step [40/46], Loss: 0.7938\n",
      "Epoch [176/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [176/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [177/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [177/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [178/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [178/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [179/200], Step [20/46], Loss: 0.7439\n",
      "Epoch [179/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [180/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [180/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [181/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [181/200], Step [40/46], Loss: 0.7439\n",
      "Epoch [182/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [182/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [183/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [183/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [184/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [184/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [185/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [185/200], Step [40/46], Loss: 0.7938\n",
      "Epoch [186/200], Step [20/46], Loss: 0.7937\n",
      "Epoch [186/200], Step [40/46], Loss: 0.7937\n",
      "Epoch [187/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [187/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [188/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [188/200], Step [40/46], Loss: 0.7437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [189/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [189/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [190/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [190/200], Step [40/46], Loss: 0.7439\n",
      "Epoch [191/200], Step [20/46], Loss: 0.7439\n",
      "Epoch [191/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [192/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [192/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [193/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [193/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [194/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [194/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [195/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [195/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [196/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [196/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [197/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [197/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [198/200], Step [20/46], Loss: 0.7457\n",
      "Epoch [198/200], Step [40/46], Loss: 0.8438\n",
      "Epoch [199/200], Step [20/46], Loss: 0.7438\n",
      "Epoch [199/200], Step [40/46], Loss: 0.7437\n",
      "Epoch [200/200], Step [20/46], Loss: 0.7437\n",
      "Epoch [200/200], Step [40/46], Loss: 0.7437\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "total_step = len(training_loader)\n",
    "print(total_step)\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(training_loader):\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        images = images.float()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)    \n",
    "#         print('loss is: ', loss.item())\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epoches, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 96.47577092511013 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testing_loader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        images = images.float()\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of the model on the test images: 98.89867841409692 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in training_loader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        images = images.float()\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Training Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
